{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport time\nfrom keras import layers, models, optimizers, callbacks, regularizers, activations, layers, initializers\nfrom keras import backend as K\nfrom PIL import Image\nfrom keras.layers import Dense, Input, Activation, Reshape, Add\nimport tensorflow as tf\nfrom keras.layers import Conv2D, BatchNormalization, Activation, Multiply, Dropout","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:39.569921Z","iopub.execute_input":"2021-08-24T09:35:39.570315Z","iopub.status.idle":"2021-08-24T09:35:43.953229Z","shell.execute_reply.started":"2021-08-24T09:35:39.570224Z","shell.execute_reply":"2021-08-24T09:35:43.952378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.set_image_data_format('channels_last')\n\nclass Mask(layers.Layer):\n    def call(self, inputs, **kwargs):\n        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n            assert len(inputs) == 2\n            inputs, mask = inputs\n        else:\n            x = K.sqrt(K.sum(K.square(inputs), -1))\n            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n\n        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n        return masked\n\n    def compute_output_shape(self, input_shape):\n        if type(input_shape[0]) is tuple:  # true label provided\n            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n        else:  # no true label provided\n            return tuple([None, input_shape[1] * input_shape[2]])\n\n    def get_config(self):\n        config = super(Mask, self).get_config()\n        return config\n\nclass PrimaryCap(layers.Layer):\n    def __init__(self, n_channels, dim_capsule, decrease_resolution=False, kernel_regularizer=None, **kwargs):\n        super(PrimaryCap, self).__init__(**kwargs)\n\n        self.n_channels = n_channels\n        self.dim_capsule = dim_capsule\n        if decrease_resolution == True:\n            self.stride = 2\n        else:\n            self.stride = 1\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        \n    def build(self, input_shape):\n        \n        self.input_height = input_shape[1]\n        self.input_width = input_shape[2]\n        self.input_num_features = input_shape[3]\n        \n        self.convW_1 = self.add_weight(shape =[3, 3, self.input_num_features, self.dim_capsule*self.n_channels],\n                                       initializer='glorot_uniform',  name='convW_1',\n                                       regularizer=self.kernel_regularizer, trainable=True)\n        self.bias_1 = self.add_weight(shape=[self.dim_capsule*self.n_channels], initializer='zeros', name='bias_1',trainable=True)\n        self.CapsAct_W = self.add_weight(shape =[1, 1, self.dim_capsule, self.dim_capsule*self.n_channels],\n                                       initializer='glorot_uniform',  name='CapsAct_W',\n                                       regularizer=self.kernel_regularizer, trainable=True)\n        self.CapsAct_B = self.add_weight(shape=[self.dim_capsule*self.n_channels], initializer='zeros', name='CapsAct_B',trainable=True)\n        \n        \n    def call(self, inputs):\n        conv1s = tf.nn.conv2d(inputs, self.convW_1, strides=[1, self.stride, self.stride, 1], padding='SAME')\n        conv1s = tf.nn.bias_add(conv1s, self.bias_1)\n        conv1s = tf.nn.relu(conv1s)\n        conv1s = tf.split(conv1s, self.n_channels, axis=-1)\n        \n        CapsAct_ws = tf.split(self.CapsAct_W, self.n_channels, axis=-1)\n        CapsAct_bs = tf.split(self.CapsAct_B, self.n_channels, axis=-1)\n        \n        def func(conv1, CapsAct_w, CapsAct_b):\n            output = tf.nn.conv2d(conv1, CapsAct_w, strides=[1, 1, 1, 1], padding='SAME')\n            output = tf.nn.bias_add(output, CapsAct_b)\n            output = tf.expand_dims(output, axis=-1)\n            return output\n        \n        outputs = [func(conv1, CapsAct_w, CapsAct_b) for conv1, CapsAct_w, CapsAct_b in zip(conv1s, CapsAct_ws, CapsAct_bs)]\n        outputs = tf.concat(outputs, axis=-1)\n        return outputs\n        \n    def compute_output_shape(self, input_shape):\n        return tuple([None, int(self.input_height/self.stride), int(self.input_width/self.stride), self.dim_capsule, self.n_channels])\n    \n    def get_config(self):\n        config = {\n            'n_channels': self.n_channels,\n            'dim_capsule': self.dim_capsule\n        }\n        base_config = super(PrimaryCap, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \nclass ConvCaps(layers.Layer):\n    def __init__(self, n_channels, dim_capsule, decrease_resolution=False, kernel_regularizer=None, **kwargs):\n        super(ConvCaps, self).__init__(**kwargs)\n\n        self.n_channels = n_channels\n        self.dim_capsule = dim_capsule\n        if decrease_resolution == True:\n            self.stride = 2\n            self.padding = 'VALID'\n        else:\n            self.stride = 1\n            self.padding = 'SAME'\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        \n    def build(self, input_shape):\n        self.height = input_shape[1]\n        self.width = input_shape[2]\n        self.input_dim = input_shape[3]\n        self.input_ch = input_shape[4]\n        \n        self.Att_W = self.add_weight(shape=[1, 1, self.dim_capsule, self.input_ch, self.input_ch*self.n_channels],\n                                    initializer='glorot_uniform',  name='Att_W',trainable=True)\n        self.ConvTrans_W = self.add_weight(shape =[3, 3, self.input_dim, self.input_ch*self.dim_capsule*self.n_channels],\n                                       initializer='glorot_uniform',  name='ConvTrans_W',\n                                       regularizer=self.kernel_regularizer, trainable=True)\n        self.ConvTrans_B = self.add_weight(shape=[self.input_ch*self.dim_capsule*self.n_channels],\n                                           initializer='zeros', name='ConvTrans_B',trainable=True)\n        self.CapsAct_W = self.add_weight(shape =[1, 1, self.dim_capsule, self.dim_capsule*self.n_channels],\n                                       initializer='glorot_uniform',  name='CapsAct_W',\n                                       regularizer=self.kernel_regularizer, trainable=True)\n        self.CapsAct_B = self.add_weight(shape=[self.dim_capsule*self.n_channels], initializer='zeros', name='CapsAct_B',trainable=True)\n        \n        \n    def call(self, inputs):\n        inputs = Dropout(rate=0.5)(inputs)\n        input_caps = tf.split(inputs, self.input_ch, axis=-1)\n        ConvTrans_ws = tf.split(self.ConvTrans_W, self.input_ch, axis=-1)\n        ConvTrans_bs = tf.split(self.ConvTrans_B, self.input_ch, axis=-1)\n        \n        # Convolutional Transform by 3x3 conv \n        conv1s = [tf.nn.conv2d(tf.squeeze(input_cap, axis=-1), ConvTrans_w, strides=[1, self.stride, self.stride, 1], padding='SAME')\n                  for input_cap, ConvTrans_w in zip(input_caps, ConvTrans_ws)]\n        conv1s = [tf.reshape(tf.nn.bias_add(conv1, ConvTrans_b), [-1, int(self.height/self.stride), int(self.width/self.stride), self.dim_capsule, self.n_channels, 1])\n                  for conv1, ConvTrans_b in zip(conv1s, ConvTrans_bs)]\n        conv1s = tf.concat(conv1s, axis=-1) \n        conv1s = tf.transpose(conv1s, [0,1,2,3,5,4])\n        \n        # Att_inputs shape : (n_ch, batch_sz, h, w, dim_cap, input_ch, 1)\n        Att_inputs = tf.split(conv1s, self.n_channels, axis=-1)\n        Att_ws = tf.split(self.Att_W, self.n_channels, axis=-1)\n        CapsAct_ws = tf.split(self.CapsAct_W, self.n_channels, axis=-1)\n        CapsAct_bs = tf.split(self.CapsAct_B, self.n_channels, axis=-1)\n        \n        def func(conv1, Att_w, CapsAct_w, CapsAct_b) :\n            x = tf.squeeze(conv1, axis=-1) #x.shape = (batch_sz, height, width, dim_cap, input_ch)\n            \n            # Attention Routing\n            # attentions shape =(batch_sz, height, width, 1, input_ch)\n            attentions = tf.nn.conv3d(x, Att_w, strides=[1, 1, 1, 1, 1], padding='VALID')\n            attentions = tf.nn.softmax(attentions, axis=-1)\n            final_attentions = Multiply()([x, attentions])\n            final_attentions = tf.reduce_sum(final_attentions, axis=-1) #final_attentions.shape = (batch_sz, height, width, dim_cap)\n            \n            conv3 = tf.nn.conv2d(final_attentions, CapsAct_w, strides=[1, 1, 1, 1], padding='SAME')\n            conv3 = tf.nn.bias_add(conv3, CapsAct_b)\n            conv3 = tf.expand_dims(conv3, axis=-1)\n            return conv3\n        \n        outputs =  [func(Att_input, Att_w, CapsAct_w, CapsAct_b) for Att_input, Att_w, CapsAct_w, CapsAct_b in \n                   zip(Att_inputs, Att_ws, CapsAct_ws, CapsAct_bs)]\n        outputs = tf.concat(outputs, axis=-1)\n        return outputs\n    \n    def compute_output_shape(self, input_shape):\n        return tuple([None, int(self.height/self.stride), int(self.width/self.stride), self.dim_capsule, self.n_channels])\n    \n    def get_config(self):\n        config = {\n            'n_channels': self.n_channels,\n            'dim_capsule': self.dim_capsule\n        }\n        base_config = super(ConvCaps, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \nclass FullyConvCaps(layers.Layer):\n    def __init__(self, n_channels, dim_capsule, kernel_regularizer=None, **kwargs):\n        super(FullyConvCaps, self).__init__(**kwargs)\n\n        self.n_channels = n_channels\n        self.dim_capsule = dim_capsule\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        \n    def build(self, input_shape):\n        self.height = input_shape[1]\n        self.width = input_shape[2]\n        self.input_dim = input_shape[3]\n        self.input_ch = input_shape[4]\n        \n        \n        self.Att_W = self.add_weight(shape=[1, 1, self.dim_capsule, self.input_ch, self.input_ch*self.n_channels],\n                                     initializer='glorot_uniform',  name='Att_W',trainable=True)\n        self.ConvTrans_W = self.add_weight(shape =[self.height, self.width, self.input_dim, self.input_ch*self.dim_capsule*self.n_channels],\n                                       initializer='glorot_uniform',  name='ConvTrans_W',\n                                       regularizer=self.kernel_regularizer, trainable=True)\n        self.ConvTrans_B = self.add_weight(shape=[self.input_ch*self.dim_capsule*self.n_channels],\n                                           initializer='zeros', name='ConvTrans_B',trainable=True)\n        self.CapsAct_W = self.add_weight(shape =[1, 1, self.dim_capsule, self.dim_capsule*self.n_channels],\n                                       initializer='glorot_uniform',  name='CapsAct_W',\n                                       regularizer=self.kernel_regularizer, trainable=True)\n        self.CapsAct_B = self.add_weight(shape=[self.dim_capsule*self.n_channels], initializer='zeros', name='CapsAct_B',trainable=True)\n        \n        \n    def call(self, inputs):\n        inputs = Dropout(rate=0.5)(inputs)\n        input_caps = tf.split(inputs, self.input_ch, axis=-1)\n        ConvTrans_ws = tf.split(self.ConvTrans_W, self.input_ch, axis=-1)\n        ConvTrans_bs = tf.split(self.ConvTrans_B, self.input_ch, axis=-1)\n                \n        # Convolutional Transform by 3x3 conv \n        conv1s = [tf.nn.conv2d(tf.squeeze(input_cap, axis=-1), ConvTrans_w, strides=[1, 1, 1, 1], padding='VALID')\n                  for input_cap, ConvTrans_w in zip(input_caps, ConvTrans_ws)]\n        conv1s = [tf.reshape(tf.nn.bias_add(conv1, ConvTrans_b), [-1, 1, 1, self.dim_capsule, self.n_channels, 1])\n                  for conv1, ConvTrans_b in zip(conv1s, ConvTrans_bs)]\n        conv1s = tf.concat(conv1s, axis=-1)\n        conv1s = tf.transpose(conv1s, [0,1,2,3,5,4])\n        \n        # Att_inputs shape : (n_ch, batch_sz, h, w, dim_cap, input_ch, 1)\n        Att_inputs = tf.split(conv1s, self.n_channels, axis=-1)\n        Att_ws = tf.split(self.Att_W, self.n_channels, axis=-1)\n        CapsAct_ws = tf.split(self.CapsAct_W, self.n_channels, axis=-1)\n        CapsAct_bs = tf.split(self.CapsAct_B, self.n_channels, axis=-1)\n        \n            \n        def func(conv1, Att_w, CapsAct_w, CapsAct_b) :\n            x = tf.squeeze(conv1, axis=-1) #x.shape = (batch_sz, height=1, width=1, dim, input_ch)\n            \n            # Attention Routing\n            # attentions shape =(batch_sz, height, width, 1, input_ch)\n            attentions = tf.nn.conv3d(x, Att_w, strides=[1, 1, 1, 1, 1], padding='VALID')\n            attentions = tf.nn.softmax(attentions, axis=-1)\n            final_attentions = Multiply()([x, attentions])\n            final_attentions = tf.reduce_sum(final_attentions, axis=-1) #final_attentions.shape = (batch_sz, height, width, dim)\n            \n            conv3 = tf.nn.conv2d(final_attentions, CapsAct_w, strides=[1, 1, 1, 1], padding='SAME')\n            conv3 = tf.nn.bias_add(conv3, CapsAct_b)\n            return conv3\n        \n        outputs = [func(Att_input, Att_w, CapsAct_w, CapsAct_b) for Att_input, Att_w, CapsAct_w, CapsAct_b in \n                   zip(Att_inputs, Att_ws, CapsAct_ws, CapsAct_bs)]\n        outputs = tf.concat(outputs, axis=-1)\n        outputs = tf.reshape(outputs, [-1, self.dim_capsule, self.n_channels])\n        outputs = tf.transpose(outputs, [0, 2, 1])\n        return outputs\n    \n    def compute_output_shape(self, input_shape):\n        return tuple([None, self.n_channels, self.dim_capsule])\n\n    def get_config(self):\n        config = {\n            'n_channels': self.n_channels,\n            'dim_capsule': self.dim_capsule\n        }\n        base_config = super(FullyConvCaps, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\ndef Conv2d_bn(input_tensor, filters, kernel_size=3, strides=1, padding='same', activation='relu', kernel_regularizer=None):\n    x = Conv2D(filters, kernel_size = kernel_size, strides= strides, padding=padding, activation=None,\n              kernel_regularizer=kernel_regularizer)(input_tensor)\n    x = BatchNormalization(axis=-1)(x)\n    x = Activation(activation)(x)\n    return x\n\n\nclass Length(layers.Layer):\n    \"\"\"\n    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n    inputs: shape=[None, num_vectors, dim_vector]\n    output: shape=[None, num_vectors]\n    \"\"\"\n    def build(self, input_shape):\n        self.dim_capsule = input_shape[-1]\n    \n    def call(self, inputs, **kwargs):\n        normalizing_tensor = tf.constant(np.sqrt(self.dim_capsule), dtype=tf.float32)\n        output = K.sqrt(K.sum(K.square(inputs), -1)) / normalizing_tensor\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-1]\n\n    def get_config(self):\n        config = super(Length, self).get_config()\n        return config\n\n\ndef margin_loss(y_true, y_pred):\n    \"\"\"\n    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n    :param y_true: [None, n_classes]\n    :param y_pred: [None, num_capsule]\n    :return: a scalar loss value.\n    \"\"\"\n    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n\n    return K.mean(K.sum(L, 1))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:43.954939Z","iopub.execute_input":"2021-08-24T09:35:43.955352Z","iopub.status.idle":"2021-08-24T09:35:44.02045Z","shell.execute_reply.started":"2021-08-24T09:35:43.955313Z","shell.execute_reply":"2021-08-24T09:35:44.019622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.set_image_data_format('channels_last')\n\ndef AR_CapsNet(input_shape, args):\n    dim_caps = int(args.dimcaps)\n    layernum = int(args.layernum)\n    print('layer num : ', layernum)\n    print('dim_caps : ', dim_caps)\n    \n    kernel_regularizer=regularizers.l2(0)\n    input_layer = Input(shape=input_shape)\n    conv1 = Conv2d_bn(input_tensor = input_layer, filters=32, kernel_size=3, strides=1, padding='same', activation='relu',\n                     kernel_regularizer=kernel_regularizer)\n    conv1 = Conv2d_bn(input_tensor = conv1, filters=32, kernel_size=3, strides=1, padding='same', activation='relu',\n                     kernel_regularizer=kernel_regularizer)\n    \n    ## Primary Capsules\n    primarycaps = PrimaryCap(n_channels=8, dim_capsule=16, decrease_resolution=True, kernel_regularizer=kernel_regularizer)(conv1)\n    primarycaps = Activation('tanh')(primarycaps)\n    print('primary caps shape : ', primarycaps.shape)\n        \n    ## Convolutional Capsules\n    if layernum == 0:\n        out = primarycaps\n    elif layernum == 1:\n        ConvCaps1 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(primarycaps)\n        ConvCaps1 = Activation('tanh')(ConvCaps1)\n        print('ConvCaps1 shape : ', ConvCaps1.shape)\n        out = ConvCaps1\n        \n    elif layernum == 2:\n        ConvCaps1 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(primarycaps)\n        ConvCaps1 = Activation('tanh')(ConvCaps1)\n        print('ConvCaps1 shape : ', ConvCaps1.shape)\n        \n        ConvCaps2 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = False, kernel_regularizer=kernel_regularizer)(ConvCaps1)\n        ConvCaps2 = Activation('tanh')(Add()([ConvCaps1 , ConvCaps2]))\n        print('ConvCaps2 shape : ', ConvCaps2.shape)\n        out = ConvCaps2\n        \n    elif layernum == 3:\n        ConvCaps1 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(primarycaps)\n        ConvCaps1 = Activation('tanh')(ConvCaps1)\n        print('ConvCaps1 shape : ', ConvCaps1.shape)\n        \n        ConvCaps2 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = False, kernel_regularizer=kernel_regularizer)(ConvCaps1)\n        ConvCaps2 = Activation('tanh')(Add()([ConvCaps1 , ConvCaps2]))\n        print('ConvCaps2 shape : ', ConvCaps2.shape)\n        \n        ConvCaps3 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = False, kernel_regularizer=kernel_regularizer)(ConvCaps2)\n        ConvCaps3 = Activation('tanh')(Add()([ConvCaps2 , ConvCaps3]))\n        print('ConvCaps3 shape : ', ConvCaps3.shape)\n        out = ConvCaps3\n        \n    ## Fully Convolutional Capsules\n    output_dim_capsule = dim_caps \n    outputs = FullyConvCaps(n_channels=2, dim_capsule=output_dim_capsule, kernel_regularizer=kernel_regularizer)(out)\n    outputs = Activation('tanh')(outputs)\n    print('Final Routing caps shape : ', outputs.shape)\n    \n    ## Length Capsules\n    real_outputs = Length()(outputs)\n    print('Length shape : ', real_outputs.shape)\n\n    n_class=2\n    y = layers.Input(shape=(n_class,))\n    masked_by_y = Mask()([outputs, y])  # The true label is used to mask the output of capsule layer. For training\n    masked = Mask()(outputs)  # Mask using the capsule with maximal length. For prediction\n\n\n    train_model = models.Model([input_layer],[real_outputs])\n\n    train_model.compile(optimizer=optimizers.SGD(lr=0.01),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return train_model","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:44.02253Z","iopub.execute_input":"2021-08-24T09:35:44.023122Z","iopub.status.idle":"2021-08-24T09:35:44.043823Z","shell.execute_reply.started":"2021-08-24T09:35:44.023052Z","shell.execute_reply":"2021-08-24T09:35:44.043043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    epochs=30\n    batch_size=32\n    debug=0\n    save_dir='./result'\n    augment='False'\n    gpu='0'\n    layernum=1\n    dimcaps=16\n    validratio=1\n    shift_fraction=0.1\nargs= args()\nprint(args)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:44.045931Z","iopub.execute_input":"2021-08-24T09:35:44.04635Z","iopub.status.idle":"2021-08-24T09:35:44.056642Z","shell.execute_reply.started":"2021-08-24T09:35:44.046313Z","shell.execute_reply":"2021-08-24T09:35:44.055816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(path):\n    dirs = os.listdir( path )\n    for item in dirs:\n        if os.path.isfile(path+item):\n            im = Image.open(path+item)\n            f, e = os.path.splitext(path+item)\n            imResize = im.resize((256,256), Image.ANTIALIAS)\n            imResize.save(f + ' resized.jpg', 'JPEG', quality=90)\n\ntrain_dir = \"../input/balanced-capsnet/Data/Train\"\nval_dir = \"../input/balanced-capsnet/Data/Test\"\nresize(os.path.join(train_dir,\"Normal\"))\nresize(os.path.join(train_dir,\"PNEUMONIA\"))\nresize(os.path.join(val_dir,\"Normal\"))\nresize(os.path.join(val_dir,\"PNEUMONIA\"))\n\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        rescale=1./255\n    )\n\ntrain_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32,color_mode='grayscale')\ntest_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False,color_mode='grayscale')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:44.059144Z","iopub.execute_input":"2021-08-24T09:35:44.05963Z","iopub.status.idle":"2021-08-24T09:35:53.681464Z","shell.execute_reply.started":"2021-08-24T09:35:44.059592Z","shell.execute_reply":"2021-08-24T09:35:53.680581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape=(256,256,1)\nmodel = AR_CapsNet(input_shape, args)\n\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:53.682743Z","iopub.execute_input":"2021-08-24T09:35:53.683302Z","iopub.status.idle":"2021-08-24T09:35:56.584301Z","shell.execute_reply.started":"2021-08-24T09:35:53.683253Z","shell.execute_reply":"2021-08-24T09:35:56.583476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(train_generator,epochs=10)\ny_true = test_generator.classes\n\nresult = model.predict_generator(test_generator)\n\ny_pred = np.argmax(result, axis=-1)\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_true, y_pred))\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T09:35:56.585545Z","iopub.execute_input":"2021-08-24T09:35:56.585872Z","iopub.status.idle":"2021-08-24T09:42:21.582529Z","shell.execute_reply.started":"2021-08-24T09:35:56.585837Z","shell.execute_reply":"2021-08-24T09:42:21.581633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}